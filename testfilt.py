import torch
import numpy as np
from PIL import Image
import os
import imageio
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from new_datasets import data_preprocessing
from model2 import SplitNeRF
import rendering
import rendering2
import logging
import cProfile

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Clear GPU cache
torch.cuda.empty_cache()

# Dataset and model parameters
data_set_path = '/home/eiyike/DATA/vanilla_dataset'
mode = 'test'
target_size = (400, 400)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
tn = 2
tf = 6

# Load dataset
logger.info("Loading test dataset...")
dataset = data_preprocessing(data_set_path, mode, target_size=target_size)
test_o, test_d, target_px_values, total_data = dataset.get_rays()


# Load trained model
logger.info("Loading trained model...")
model = torch.load('/home/eiyike/New_Vanilla_Nerf/nerf_final_model3.pth').to(device)
model.eval()  # Set model to evaluation mode

# Function to convert MSE to PSNR
def mse2psnr(mse):
    return 20 * np.log10(1 / np.sqrt(mse))

@torch.no_grad()
def test(model, ray_origins, ray_directions, tn, tf, nb_bins=100, chunk_size=10, H=400, W=400, target=None):
    """
    Render images using the NeRF model and compute evaluation metrics.

    Args:
        model (torch.nn.Module): Trained NeRF model.
        ray_origins (torch.Tensor): Ray origins of shape (num_rays, 3).
        ray_directions (torch.Tensor): Ray directions of shape (num_rays, 3).
        tn (float): Near bound for ray sampling.
        tf (float): Far bound for ray sampling.
        nb_bins (int): Number of bins for hierarchical sampling.
        chunk_size (int): Number of rays to process at once.
        H (int): Height of the rendered image.
        W (int): Width of the rendered image.
        target (np.ndarray): Ground truth image for evaluation.

    Returns:
        image (np.ndarray): Rendered image of shape (H, W, 3).
        mse (float): Mean squared error between the rendered and target images.
        psnr (float): Peak signal-to-noise ratio between the rendered and target images.
    """
    ray_origins = ray_origins.chunk(chunk_size)
    ray_directions = ray_directions.chunk(chunk_size)
    xyzs=[]
    image = []
    for o_batch, d_batch in zip(ray_origins, ray_directions):
        img_batch = rendering.rendering(model, o_batch, d_batch, tn, tf, nb_bins=nb_bins, device=o_batch.device)
        xyz=rendering2.rendering(model, o_batch, d_batch, tn, tf, nb_bins=nb_bins, device=o_batch.device)
        xyzs.append(xyz)
        image.append(img_batch)  # [chunk_size, 3]
    image = torch.cat(image)
    image = image.reshape(H, W, 3).cpu().numpy()
    xyzs = torch.cat(xyzs)
    xyzs = xyzs.reshape(H, W, 100, 3).cpu().numpy()
    # breakpoint()

    if target is not None:
        mse = ((image - target) ** 2).mean()
        psnr = mse2psnr(mse)
        return image, xyzs, mse, psnr
    else:
        return image, xyzs

# Profile the test function
with cProfile.Profile() as pr:
    img, xyzs, mse, psnr = test(
        model,
        torch.from_numpy(test_o[4]).to(device).float(),
        torch.from_numpy(test_d[4]).to(device).float(),
        tn, tf, nb_bins=100, chunk_size=10, target=target_px_values[4].reshape(400, 400, 3)
    )
    pr.dump_stats('prof_test_objectscene10_epochs.prof')

# Log evaluation metrics



logger.info(f"MSE: {mse:.6f}, PSNR: {psnr:.2f} dB")
z_index = 50  # You can change this to any value between 0 and 99

# Choose which component to visualize (0, 1, or 2 from the last dimension)
component = 2  # Change to 1 or 2 to see other components

# Extract a 2D slice
slice_2d = xyzs[:, :, z_index, component]

# Create the plot
plt.figure(figsize=(10, 8))
plt.imshow(slice_2d, cmap='viridis')
plt.colorbar(label=f'Component {component} value')
plt.title(f'2D Slice at z_index={z_index}')
plt.xlabel('X')
plt.ylabel('Y')
plt.tight_layout()
plt.show()

# Alternatively, you could visualize an average across the 3rd dimension
avg_2d = np.mean(xyzs[:, :, :, component], axis=2)

plt.figure(figsize=(10, 8))
plt.imshow(avg_2d, cmap='plasma')
plt.colorbar(label=f'Average of component {component}')
plt.title(f'Average of Component {component} Across Z Dimension')
plt.xlabel('X')
plt.ylabel('Y')
plt.tight_layout()
plt.show()
# breakpoint()
# Save and display the rendered image
plt.imshow(img)
plt.title(f"Image Generated by NeRF (PSNR: {psnr:.2f} dB)")
plt.savefig("dragon5555.png")
plt.show()

fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')

# The full dataset has 400*400*100 = 16,000,000 points, which is too many to plot
# Let's subsample to make the visualization manageable
# Taking every 20th point in each dimension
step = 20
x = xyzs[::step, ::step, ::step, 0].flatten()
y = xyzs[::step, ::step, ::step, 1].flatten()
z = xyzs[::step, ::step, ::step, 2].flatten()

# Create the 3D scatter plot
scatter = ax.scatter(x, y, z, c=z, cmap='viridis', s=5, alpha=0.8)

# Add labels and colorbar
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
plt.colorbar(scatter, ax=ax, label='Z value')
plt.title('3D Scatter Plot of Points (Subsampled)')

# Show the plot
plt.tight_layout()
plt.show()



# Optional: Render an image using the trained model
if False:  # Set to True to enable rendering
    logger.info("Rendering test image...")
    img = rendering.rendering(
        model,
        torch.from_numpy(ray_origins[0]).type(torch.float32).to(device),
        torch.from_numpy(ray_directions[0]).type(torch.float32).to(device),
        tn, tf, nb_bins=100, device=device
    )
    plt.imshow(img.reshape(size_h, size_w, 3).data.cpu().numpy())
    plt.savefig("rendered_image.png")
    plt.show()
    logger.info("Rendered image saved to 'rendered_image.png'.")